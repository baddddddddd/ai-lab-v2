#!/usr/bin/env python3
"""
!!! Code generated by Claude Sonnet 4 !!!

Step Law: Optimal Hyperparameter Scaling Law for Large Language Model Pretraining

Based on the research paper: "Predictable Scale: Part I -- Optimal Hyperparameter 
Scaling Law in Large Language Model Pretraining"

The Step Law provides universal scaling formulas:
- Learning Rate: η(N, D) = 1.79 * N^(-0.713) * D^(0.307)
- Batch Size: B(D) = 0.58 * D^(0.571)

Where:
- N: Number of non-embedding parameters
- D: Dataset size in tokens
"""

import math
import argparse
import json
from typing import Dict, Tuple, Optional, List
import matplotlib.pyplot as plt
import numpy as np


class StepLawOptimizer:
    """
    Implementation of the Step Law for optimal LLM hyperparameter prediction.
    """

    # Step Law coefficients from the paper
    LR_COEFF_C = 1.79
    LR_COEFF_ALPHA = -0.713
    LR_COEFF_BETA = 0.307

    BS_COEFF_D = 0.58
    BS_COEFF_GAMMA = 0.571

    # Recommended fixed final learning rate
    FIXED_FINAL_LR = 1e-5

    def __init__(self):
        """Initialize the Step Law optimizer."""
        self.history = []

    def calculate_optimal_lr(self, num_parameters: float, dataset_size: float) -> float:
        """
        Calculate optimal learning rate using Step Law.

        Args:
            num_parameters: Number of non-embedding parameters (N)
            dataset_size: Dataset size in tokens (D)

        Returns:
            Optimal peak learning rate
        """
        lr = (
            self.LR_COEFF_C
            * (num_parameters**self.LR_COEFF_ALPHA)
            * (dataset_size**self.LR_COEFF_BETA)
        )
        return lr

    def calculate_optimal_batch_size(self, dataset_size: float) -> int:
        """
        Calculate optimal batch size using Step Law.

        Args:
            dataset_size: Dataset size in tokens (D)

        Returns:
            Optimal batch size in tokens
        """
        batch_size = self.BS_COEFF_D * (dataset_size**self.BS_COEFF_GAMMA)
        return int(round(batch_size))

    def optimize_hyperparameters(
        self, num_parameters: float, dataset_size: float
    ) -> Dict[str, float]:
        """
        Calculate optimal hyperparameters for given model and dataset size.

        Args:
            num_parameters: Number of non-embedding parameters
            dataset_size: Dataset size in tokens

        Returns:
            Dictionary containing optimal hyperparameters
        """
        optimal_lr = self.calculate_optimal_lr(num_parameters, dataset_size)
        optimal_bs = self.calculate_optimal_batch_size(dataset_size)

        result = {
            "num_parameters": num_parameters,
            "dataset_size": dataset_size,
            "optimal_learning_rate": optimal_lr,
            "optimal_batch_size": optimal_bs,
            "fixed_final_lr": self.FIXED_FINAL_LR,
            "lr_decay_ratio": optimal_lr / self.FIXED_FINAL_LR,
        }

        # Store in history
        self.history.append(result)

        return result

    def compare_with_baselines(
        self, num_parameters: float, dataset_size: float
    ) -> Dict[str, Dict[str, float]]:
        """
        Compare Step Law predictions with other scaling law baselines.

        Args:
            num_parameters: Number of non-embedding parameters
            dataset_size: Dataset size in tokens

        Returns:
            Dictionary comparing different scaling laws
        """
        # Step Law (our method)
        step_law = self.optimize_hyperparameters(num_parameters, dataset_size)

        # OpenAI Law (Kaplan et al., 2020)
        # LR = 3.239 * 10^-3 + -1.395 * 10^-4 * log(N)
        openai_lr = 3.239e-3 - 1.395e-4 * math.log(num_parameters)
        openai_bs = 2e18 / (dataset_size**4.76190)  # Simplified approximation

        # Microsoft Law (Bjorck et al., 2024)
        # LR = 1.3192e-5 * N^-0.23 * D^-0.32
        microsoft_lr = 1.3192e-5 * (num_parameters**-0.23) * (dataset_size**-0.32)

        # DeepSeek Law (DeepSeek-AI et al., 2024)
        # Using compute budget C approximation
        C = 6 * num_parameters * dataset_size  # FLOPs approximation
        deepseek_lr = 0.3188 * (C**-0.1250)
        deepseek_bs = int(0.2920 * (C**0.3271))

        # Porian Law (Porian et al., 2024)
        porian_lr = 3.7 * (num_parameters**-0.36)
        porian_bs = int(0.7576 * (num_parameters**0.703))

        return {
            "Step Law (Ours)": {
                "learning_rate": step_law["optimal_learning_rate"],
                "batch_size": step_law["optimal_batch_size"],
                "method": "Step Law",
            },
            "OpenAI Law": {
                "learning_rate": max(openai_lr, 1e-6),  # Prevent negative LR
                "batch_size": max(int(openai_bs), 1024),  # Reasonable minimum
                "method": "OpenAI",
            },
            "Microsoft Law": {
                "learning_rate": microsoft_lr,
                "batch_size": step_law["optimal_batch_size"],  # MS doesn't predict BS
                "method": "Microsoft",
            },
            "DeepSeek Law": {
                "learning_rate": deepseek_lr,
                "batch_size": deepseek_bs,
                "method": "DeepSeek",
            },
            "Porian Law": {
                "learning_rate": porian_lr,
                "batch_size": porian_bs,
                "method": "Porian",
            },
        }

    def generate_scaling_curve(
        self,
        parameter_range: Tuple[float, float],
        dataset_size: float,
        num_points: int = 50,
    ) -> Dict[str, List[float]]:
        """
        Generate scaling curves for visualization.

        Args:
            parameter_range: (min_params, max_params) range
            dataset_size: Fixed dataset size
            num_points: Number of points to generate

        Returns:
            Dictionary with parameter counts and corresponding optimal values
        """
        min_params, max_params = parameter_range
        param_counts = np.logspace(
            math.log10(min_params), math.log10(max_params), num_points
        )

        learning_rates = []
        batch_sizes = []

        for params in param_counts:
            lr = self.calculate_optimal_lr(params, dataset_size)
            bs = self.calculate_optimal_batch_size(dataset_size)
            learning_rates.append(lr)
            batch_sizes.append(bs)

        return {
            "parameter_counts": param_counts.tolist(),
            "learning_rates": learning_rates,
            "batch_sizes": batch_sizes,
        }

    def plot_scaling_curves(
        self,
        parameter_range: Tuple[float, float],
        dataset_sizes: List[float],
        save_path: Optional[str] = None,
    ):
        """
        Plot scaling curves for different dataset sizes.

        Args:
            parameter_range: (min_params, max_params) range
            dataset_sizes: List of dataset sizes to plot
            save_path: Optional path to save the plot
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        colors = plt.cm.viridis(np.linspace(0, 1, len(dataset_sizes)))

        for i, dataset_size in enumerate(dataset_sizes):
            data = self.generate_scaling_curve(parameter_range, dataset_size)

            # Learning Rate plot
            ax1.loglog(
                data["parameter_counts"],
                data["learning_rates"],
                color=colors[i],
                label=f"D={dataset_size/1e9:.1f}B tokens",
                linewidth=2,
            )

            # Batch Size plot (constant for each dataset size)
            bs_constant = data["batch_sizes"][0]  # Same for all param counts
            ax2.loglog(
                data["parameter_counts"],
                [bs_constant] * len(data["parameter_counts"]),
                color=colors[i],
                label=f"D={dataset_size/1e9:.1f}B tokens",
                linewidth=2,
            )

        # Configure Learning Rate plot
        ax1.set_xlabel("Model Parameters (N)")
        ax1.set_ylabel("Optimal Learning Rate")
        ax1.set_title("Step Law: Learning Rate Scaling")
        ax1.grid(True, alpha=0.3)
        ax1.legend()

        # Configure Batch Size plot
        ax2.set_xlabel("Model Parameters (N)")
        ax2.set_ylabel("Optimal Batch Size (tokens)")
        ax2.set_title("Step Law: Batch Size Scaling")
        ax2.grid(True, alpha=0.3)
        ax2.legend()

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
            print(f"Plot saved to {save_path}")

        plt.show()

    def export_results(self, filename: str):
        """Export optimization history to JSON file."""
        with open(filename, "w") as f:
            json.dump(self.history, f, indent=2)
        print(f"Results exported to {filename}")

    def print_summary(self, result: Dict[str, float]):
        """Print a formatted summary of optimization results."""
        print("\n" + "=" * 60)
        print("STEP LAW HYPERPARAMETER OPTIMIZATION RESULTS")
        print("=" * 60)
        print(f"Model Parameters (N):      {result['num_parameters']:,.0f}")
        print(f"Dataset Size (D):          {result['dataset_size']:,.0f} tokens")
        print(f"                          {result['dataset_size']/1e9:.2f}B tokens")
        print("-" * 60)
        print(f"Optimal Learning Rate:     {result['optimal_learning_rate']:.6f}")
        print(f"Optimal Batch Size:        {result['optimal_batch_size']:,} tokens")
        print(f"Fixed Final LR:            {result['fixed_final_lr']:.0e}")
        print(f"LR Decay Ratio:            {result['lr_decay_ratio']:.1f}x")
        print("=" * 60)

        # Add training recommendations
        print("\nTRAINING RECOMMENDATIONS:")
        print(
            f"• Use cosine decay from {result['optimal_learning_rate']:.6f} to {result['fixed_final_lr']:.0e}"
        )
        print(f"• Batch size: {result['optimal_batch_size']:,} tokens")
        print(f"• Warmup: Linear warmup over first 2,000 steps")
        print(f"• Optimizer: AdamW (β1=0.9, β2=0.95, ε=1e-8, weight_decay=0.1)")


def main():
    """Command-line interface for the Step Law optimizer."""
    parser = argparse.ArgumentParser(
        description="Step Law: Optimal Hyperparameter Scaling for LLM Pretraining",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Optimize hyperparameters for a 1B parameter model on 100B tokens
  python step_law.py --params 1e9 --tokens 100e9
  
  # Compare with baselines
  python step_law.py --params 1e9 --tokens 100e9 --compare
  
  # Generate scaling plots
  python step_law.py --plot --min-params 1e8 --max-params 1e11 --dataset-sizes 1e9,10e9,100e9
        """,
    )

    parser.add_argument("--params", type=float, help="Number of model parameters (N)")
    parser.add_argument("--tokens", type=float, help="Dataset size in tokens (D)")
    parser.add_argument(
        "--compare", action="store_true", help="Compare with baseline methods"
    )
    parser.add_argument("--plot", action="store_true", help="Generate scaling plots")
    parser.add_argument(
        "--min-params", type=float, default=1e8, help="Minimum parameters for plotting"
    )
    parser.add_argument(
        "--max-params", type=float, default=1e11, help="Maximum parameters for plotting"
    )
    parser.add_argument(
        "--dataset-sizes",
        type=str,
        default="1e9,10e9,100e9",
        help="Comma-separated dataset sizes for plotting",
    )
    parser.add_argument("--export", type=str, help="Export results to JSON file")
    parser.add_argument("--save-plot", type=str, help="Save plot to file")

    args = parser.parse_args()

    optimizer = StepLawOptimizer()

    if args.plot:
        dataset_sizes = [float(x) for x in args.dataset_sizes.split(",")]
        optimizer.plot_scaling_curves(
            (args.min_params, args.max_params), dataset_sizes, args.save_plot
        )
        return

    if args.params is None or args.tokens is None:
        print("Error: Both --params and --tokens are required for optimization.")
        print("Use --help for usage information.")
        return

    # Optimize hyperparameters
    result = optimizer.optimize_hyperparameters(args.params, args.tokens)
    optimizer.print_summary(result)

    # Compare with baselines if requested
    if args.compare:
        print("\n" + "=" * 60)
        print("COMPARISON WITH BASELINE METHODS")
        print("=" * 60)
        comparisons = optimizer.compare_with_baselines(args.params, args.tokens)

        for method, values in comparisons.items():
            print(f"\n{method}:")
            print(f"  Learning Rate: {values['learning_rate']:.6f}")
            print(f"  Batch Size:    {values['batch_size']:,}")

    # Export results if requested
    if args.export:
        optimizer.export_results(args.export)


if __name__ == "__main__":
    # If run without arguments, show example usage
    import sys

    if len(sys.argv) == 1:
        print("Step Law: Optimal Hyperparameter Scaling for LLM Pretraining")
        print("\nExample usage:")

        # Create optimizer and show example
        optimizer = StepLawOptimizer()

        # Example: 1B parameter model, 100B tokens
        result = optimizer.optimize_hyperparameters(1e9, 100e9)
        optimizer.print_summary(result)

        print("\nFor more options, run with --help")
        print("To generate plots: python step_law.py --plot")
        print(
            "To compare methods: python step_law.py --params 1e9 --tokens 100e9 --compare"
        )
    else:
        main()
